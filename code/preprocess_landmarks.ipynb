{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LZV .tfrecord datu sagatavošana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Šī Jupyter grāmatiņa sagatavo training.tfrecord un validation.tfrecord, izmantojot no video failiem iegūtos orientieru failus, kas saglabāti .npy formātā. Šajā grāmatiņā arī tiek veikta normalizācija.\n",
    "\n",
    "Lai palaistu šo grāmatiņu, ir nepieciešams:\n",
    "\n",
    "* GitHub repo sagatavotais:\n",
    "    * char_map.json\n",
    "\n",
    "* Jūsu pašu sagatavots:\n",
    "    * .npy formātā saglabāti orientieru masīvi, kas atrodas `data\\training_landmarks`\n",
    "\n",
    "Kā iegūt MediaPipe Holistic orientierus no video failiem, var redzēt MediaPipe dokumentācijā.\n",
    "\n",
    "Jums ir jāsagatavo dati struktūrā, kur apakšmapīšu un failu nosaukumi atbilst oriģinālvideo parādītajām zīmēm. Indeksācija faila nosaukumā nav svarīga datu apstrādē, tā ir domāta tikai vienkāršākai failu pārskatei.\n",
    "\n",
    "Piemērs struktūrai:\n",
    "\n",
    "* `data\\training_landmarks\\d-z-i-m-š-a-n-a-s- -d-i-e-n-a\\d-z-i-m-š-a-n-a-s- -d-i-e-n-a_0001.npy`\n",
    "\n",
    "* `data\\training_landmarks\\k-ā- -t-e-v-i- -s-a-u-c\\k-ā- -t-e-v-i- -s-a-u-c_0001.npy`\n",
    "\n",
    "* `data\\training_landmarks\\g-u-n-t-a\\g-u-n-t-a_0001.npy`\n",
    "\n",
    "* `data\\training_landmarks\\g-u-n-t-a\\g-u-n-t-a_0002.npy`\n",
    "\n",
    "Piemērs `g-u-n-t-a_0001.npy` ir dots.\n",
    "\n",
    "Orientieriem ir 3 dimensijas (x,y,z) un to globālā indeksācija ir šāda:\n",
    "* 33 pozas (0-32)\n",
    "* 468 sejas (33-500)\n",
    "* 21 kreisās rokas (501-521)\n",
    "* 21 labās rokas (522-542)\n",
    "\n",
    "Galā tiks iegūts training.tfrecord un validation.tfrecord, kā arī metadatu fails (kam vajadzētu tikt pārrakstītam ar Jūsu sagatavoto datu informāciju)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Konfigurācija\n",
    "\n",
    "Nepieciešamie faili un parametri. Šeit ir nepieciešams norādīt savu vēlamo trenēšanas/validācijas sadalīšanu, kas šobrīd ir 80 training : 20 validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmark count: 543\n",
      "Char map size: 45\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "\n",
    "    DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "    CHAR_MAP_FILE = os.path.join(DATA_DIR, \"processed_landmarks\", \"char_map.json\")\n",
    "    DATASET_INFO_FILE = os.path.join(DATA_DIR, \"processed_landmarks\", \"dataset_info.json\")\n",
    "    TRAINING_LANDMARKS_DIR = os.path.join(DATA_DIR, \"processed_landmarks\", \"training_landmarks\")\n",
    "\n",
    "    # Orientieru konfigurācija\n",
    "    LANDMARK_DIMS = 3\n",
    "    POSE_IDXS = list(range(0, 33))\n",
    "    FACE_IDXS = list(range(33, 501))\n",
    "    LHAND_IDXS = list(range(501, 522))\n",
    "    RHAND_IDXS = list(range(522, 543))\n",
    "    ALL_LANDMARKS = POSE_IDXS + FACE_IDXS + LHAND_IDXS + RHAND_IDXS\n",
    "    NUM_NODES = len(ALL_LANDMARKS)\n",
    "    print(\"Landmark count:\", NUM_NODES)\n",
    "\n",
    "    VALIDATION_SPLIT_SIZE = 0.20 # Izvēlēties savu trenēšanas/validācijas datu daļu, šobrīd 80 training : 20 validation\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    # Char map\n",
    "    with open(CHAR_MAP_FILE, 'r', encoding='utf-8') as f:\n",
    "        char_map = json.load(f)\n",
    "        id_to_char = {int(v): k for k, v in char_map.items()}\n",
    "\n",
    "    inverse_char_map = {}\n",
    "    try:\n",
    "        inverse_char_map = {v: k for k, v in char_map.items()}\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"Char map size:\", len(char_map))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Palīgfunkcijas\n",
    "\n",
    "Dažādas palīgfunkcijas, kas nepieciešamas priekšapstrādes veikšanai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadatu izveides funkcija\n",
    "def create_metadata(DATA_DIR):\n",
    "    samples = []\n",
    "\n",
    "    for root, _, files in os.walk(DATA_DIR):\n",
    "        for fname in files:\n",
    "            if fname.endswith(\".npy\"):\n",
    "                file_id = fname[:-4] # Sagaidāmās struktūras piemērs \"5- -f-ū-r-e-s_0001\"\n",
    "                parts = file_id.split('_', 1)\n",
    "                sequence_str = parts[0]\n",
    "                signs = sequence_str.split(\"-\")\n",
    "\n",
    "                valid_sequence = []\n",
    "                valid = True\n",
    "                for sign in signs:\n",
    "                    if sign in char_map:\n",
    "                        valid_sequence.append(sign)\n",
    "                    else:\n",
    "                        print(f\"    Warning: Sign '{sign}' from file '{fname}' (sequence: '{sequence_str}') not found in signs.csv. Skipping file.\")\n",
    "                        valid = False\n",
    "                        break\n",
    "\n",
    "                if valid:\n",
    "                    samples.append({\n",
    "                        \"file_id\": file_id,\n",
    "                        \"path\": os.path.join(root, fname),\n",
    "                        \"phrase\": valid_sequence # Saglabā zīmes kā sarakstu\n",
    "                    })\n",
    "    if not samples:\n",
    "        return pd.DataFrame()\n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "# Aprēķina statistiku par zīmēm\n",
    "def calculate_stats(df):\n",
    "    pose_landmarks_all, face_landmarks_all, lhand_landmarks_all, rhand_landmarks_all = [], [], [], []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading landmarks for stats\"):\n",
    "        try:\n",
    "            landmarks = np.load(row['path'])\n",
    "            if landmarks.ndim != 3 or landmarks.shape[1] != NUM_NODES or landmarks.shape[2] != LANDMARK_DIMS:\n",
    "                print(f\"\\nWarning: Skipping file {row['path']} due to unexpected shape: {landmarks.shape}. Expected: (frames, {NUM_NODES}, {LANDMARK_DIMS})\")\n",
    "                continue\n",
    "\n",
    "            pose = landmarks[:, POSE_IDXS, :]\n",
    "            face = landmarks[:, FACE_IDXS, :]\n",
    "            lhand = landmarks[:, LHAND_IDXS, :]\n",
    "            rhand = landmarks[:, RHAND_IDXS, :]\n",
    "\n",
    "            def filter_and_clean(data):\n",
    "                if data.shape[0] == 0: return data\n",
    "                valid_frames_mask = (~np.isnan(data).all(axis=(1,2))) & ((data != 0).any(axis=(1,2)))\n",
    "                filtered_data = data[valid_frames_mask]\n",
    "                cleaned_data = np.nan_to_num(filtered_data)\n",
    "                return cleaned_data\n",
    "\n",
    "            pose_clean = filter_and_clean(pose)\n",
    "            face_clean = filter_and_clean(face)\n",
    "            lhand_clean = filter_and_clean(lhand)\n",
    "            rhand_clean = filter_and_clean(rhand)\n",
    "\n",
    "            if pose_clean.shape[0] > 0: pose_landmarks_all.append(pose_clean)\n",
    "            if face_clean.shape[0] > 0: face_landmarks_all.append(face_clean)\n",
    "            if lhand_clean.shape[0] > 0: lhand_landmarks_all.append(lhand_clean)\n",
    "            if rhand_clean.shape[0] > 0: rhand_landmarks_all.append(rhand_clean)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "\n",
    "    stats = {}\n",
    "    EPSILON = 1e-8 # Lai izvairītos no nulles dalīšanas kļūdām\n",
    "    calculated_something = False\n",
    "\n",
    "    for part_name, part_data_list, part_indices in [\n",
    "        ('pose', pose_landmarks_all, POSE_IDXS),\n",
    "        ('face', face_landmarks_all, FACE_IDXS),\n",
    "        ('lhand', lhand_landmarks_all, LHAND_IDXS),\n",
    "        ('rhand', rhand_landmarks_all, RHAND_IDXS)\n",
    "    ]:\n",
    "        expected_shape = (len(part_indices), LANDMARK_DIMS)\n",
    "        if part_data_list:\n",
    "            all_part_data = np.concatenate(part_data_list, axis=0)\n",
    "            mean_val = all_part_data.mean(axis=0)\n",
    "            std_val = all_part_data.std(axis=0)\n",
    "            \n",
    "            if mean_val.shape != expected_shape or std_val.shape != expected_shape:\n",
    "                print(f\"   Warning: Shape mismatch for {part_name}. Mean: {mean_val.shape}, Std: {std_val.shape}, Expected: {expected_shape}\")\n",
    "            stats[f'{part_name}_mean'] = mean_val\n",
    "            stats[f'{part_name}_std'] = std_val + EPSILON\n",
    "            calculated_something = True\n",
    "        else:\n",
    "            print(f\"   Warning: No valid data found for {part_name}. Using default stats (mean=0, std=1).\")\n",
    "            stats[f'{part_name}_mean'] = np.zeros(expected_shape)\n",
    "            stats[f'{part_name}_std'] = np.ones(expected_shape)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def normalize_landmarks(landmarks, stats_dict):\n",
    "    normalized = np.zeros_like(landmarks, dtype=np.float32)\n",
    "    if not stats_dict:\n",
    "        print(\"Warning: Stats dictionary is empty, returning unnormalized data.\")\n",
    "        return landmarks.astype(np.float32)\n",
    "\n",
    "    try:\n",
    "        normalized[:, POSE_IDXS, :] = (landmarks[:, POSE_IDXS, :] - stats_dict['pose_mean'][np.newaxis, :, :]) / stats_dict['pose_std'][np.newaxis, :, :]\n",
    "        normalized[:, FACE_IDXS, :] = (landmarks[:, FACE_IDXS, :] - stats_dict['face_mean'][np.newaxis, :, :]) / stats_dict['face_std'][np.newaxis, :, :]\n",
    "        normalized[:, LHAND_IDXS, :] = (landmarks[:, LHAND_IDXS, :] - stats_dict['lhand_mean'][np.newaxis, :, :]) / stats_dict['lhand_std'][np.newaxis, :, :]\n",
    "        normalized[:, RHAND_IDXS, :] = (landmarks[:, RHAND_IDXS, :] - stats_dict['rhand_mean'][np.newaxis, :, :]) / stats_dict['rhand_std'][np.newaxis, :, :]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}.\")\n",
    "        return landmarks.astype(np.float32)\n",
    "\n",
    "    normalized = np.nan_to_num(normalized)\n",
    "    normalized = np.clip(normalized, np.finfo(np.float32).min / 100, np.finfo(np.float32).max / 100) \n",
    "    return normalized.astype(np.float32)\n",
    "\n",
    "# Palīgfunkcijas TFRecord datu ierakstiem\n",
    "def float_feature(value):\n",
    "    if not isinstance(value, np.ndarray): value = np.array(value, dtype=np.float32)\n",
    "    if value.dtype != np.float32: value = value.astype(np.float32)\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.tobytes()]))\n",
    "\n",
    "def int_feature(value):\n",
    "    if not isinstance(value, list): value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "# Izveido TFRecord failus\n",
    "def create_tfrecords(df, output_path, stats_dict, normalize=True):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    print(f\"\\nCreating TFRecord file: {output_path}\")\n",
    "    skipped_count = 0\n",
    "    processed_count = 0\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Writing {os.path.basename(output_path)}\"):\n",
    "        try:\n",
    "            landmarks = np.load(row['path'])\n",
    "            if landmarks.ndim != 3 or landmarks.shape[1] != NUM_NODES or landmarks.shape[2] != LANDMARK_DIMS:\n",
    "                print(f\"\\nWarning: Skipping file {row['path']} due to unexpected shape: {landmarks.shape}. Expected (frames, {NUM_NODES}, {LANDMARK_DIMS})\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            landmarks_to_save = landmarks.astype(np.float32)\n",
    "\n",
    "            if normalize:\n",
    "                landmarks_to_save = normalize_landmarks(landmarks_to_save, stats_dict)\n",
    "\n",
    "            if np.isnan(landmarks_to_save).any(): # Pārbauda, vai ir NaN vērtības\n",
    "                print(f\"\\nWarning: NaNs found in data for {row['path']} after processing! Skipping record.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            phrase_list = row['phrase']\n",
    "            phrase_indices = [char_map[token] for token in phrase_list]\n",
    "\n",
    "            # Struktūra ierakstiem\n",
    "            feature = {\n",
    "                'landmarks': float_feature(landmarks_to_save),\n",
    "                'phrase': int_feature(phrase_indices),\n",
    "                'length': int_feature(landmarks_to_save.shape[0]),\n",
    "                'phrase_length': int_feature(len(phrase_indices)),\n",
    "            }\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            writer.write(example.SerializeToString())\n",
    "            processed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {type(e).__name__} - {e}\")\n",
    "            skipped_count += 1\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Finished creating TFRecord: {output_path}\")\n",
    "    print(f\"  Processed: {processed_count} records\")\n",
    "    print(f\"  Skipped: {skipped_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Galvenā apstrāde\n",
    "\n",
    "Veic datu apstrādi. Saglabā training.tfrecord un validation.tfrecord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_df = create_metadata(TRAINING_LANDMARKS_DIR)\n",
    "print(initial_train_df)\n",
    "training_stats_for_test = {}\n",
    "\n",
    "if not initial_train_df.empty:\n",
    "    print(f\"Initial training data: {len(initial_train_df)} samples.\")\n",
    "    \n",
    "    actual_train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "\n",
    "    if len(initial_train_df) * VALIDATION_SPLIT_SIZE < 1 or len(initial_train_df) < 2 : # Ja gadījumā nav vismaz 2 paraugi validācijas dalīšanai\n",
    "        print(\"Warning: Dataset too small for validation split or less than 2 samples. Using all data for training.\")\n",
    "        actual_train_df = initial_train_df.copy()\n",
    "        # Šajā gadījumā nebūs validācijas datu\n",
    "    else:\n",
    "        actual_train_df, val_df = train_test_split(\n",
    "            initial_train_df,\n",
    "            test_size=VALIDATION_SPLIT_SIZE,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=None\n",
    "        )\n",
    "        print(f\"Split complete: {len(actual_train_df)} training samples, {len(val_df)} validation samples.\")\n",
    "\n",
    "    # Aprēķina statistiku par treniņu datiem\n",
    "    if not actual_train_df.empty:\n",
    "        training_stats = calculate_stats(actual_train_df)\n",
    "        training_stats_for_test = training_stats\n",
    "\n",
    "        create_tfrecords(actual_train_df, os.path.join(DATA_DIR, \"processed_landmarks\", \"training.tfrecord\"), training_stats, normalize=True)\n",
    "        \n",
    "        if not val_df.empty:\n",
    "            create_tfrecords(val_df, os.path.join(\"validation.tfrecord\"), training_stats, normalize=True)\n",
    "        else:\n",
    "            print(\"No validation data to process into TFRecord.\")\n",
    "    else:\n",
    "        print(\"No training samples after split (or initial dataset was too small). Skipping stats calculation and TFRecord creation.\")\n",
    "\n",
    "else:\n",
    "    print(\"No training data found or processed. Skipping training, validation, and testing TFRecord creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Metadatu faila izveide\n",
    "\n",
    "Izveido vai pārraksta metadatu failu `dataset_info.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Saglabā jaunu char_map (utf-8 priekš diakritiskajām zīmēm)\n",
    "# with open(CHAR_MAP_FILE, \"w\", encoding='utf-8') as f:\n",
    "#     json.dump(char_map, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Saglabātā metadatu informācija\n",
    "dataset_info = {\n",
    "    \"dataset_name\": \"LZV\",\n",
    "    \"data files\": {\n",
    "        \"train_tfrecord\": \"training.tfrecord\",\n",
    "        \"validation_tfrecord\": \"validation.tfrecord\",\n",
    "        \"char_map\": \"char_map.json\",\n",
    "        \"training_stats_dir\": \"training_stats\"\n",
    "    },\n",
    "    \"num_classes\": len(char_map),\n",
    "    \"num_classes_with_blank\": len(char_map)+1,\n",
    "    \"dataset_stats\": {\n",
    "        \"train_samples\": len(actual_train_df) if not actual_train_df.empty else 0,\n",
    "        \"val_samples\": len(val_df) if not val_df.empty else 0,\n",
    "        \"total_samples\": (len(actual_train_df) if not actual_train_df.empty else 0) + (len(val_df) if not val_df.empty else 0)\n",
    "    },\n",
    "    \"landmark_info\": {\n",
    "        \"source_tool\": \"MediaPipe Holistic\",\n",
    "        \"num_landmarks\": NUM_NODES,\n",
    "        \"num_coordinates\": LANDMARK_DIMS,\n",
    "        \"input_shape\": [None, NUM_NODES, LANDMARK_DIMS],\n",
    "        \"landmark_components\": {\n",
    "            \"pose\": [POSE_IDXS[0], POSE_IDXS[-1]],\n",
    "            \"face\": [FACE_IDXS[0], FACE_IDXS[-1]],\n",
    "            \"lhand\": [LHAND_IDXS[0], LHAND_IDXS[-1]],\n",
    "            \"rhand\": [RHAND_IDXS[0], RHAND_IDXS[-1]]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Saglabā (utf-8 priekš diakritiskajām zīmēm)\n",
    "with open(DATASET_INFO_FILE, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(dataset_info, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pārbaude\n",
    "\n",
    "Papildus datu pārbaude, kas izvada dažu datu struktūru. Izvadīto daudzumu var izmainīt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying: ../data/processed_landmarks/training.tfrecord\n",
      "\n",
      "--- Verifying first 5 examples ---\n",
      "\n",
      "--- Example 1 ---\n",
      "Landmarks shape: (39, 543, 3)\n",
      "Phrase: ['o']\n",
      "Landmark Sequence Length (Frames): 39\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 2 ---\n",
      "Landmarks shape: (147, 543, 3)\n",
      "Phrase: ['9', '8', '9', '8']\n",
      "Landmark Sequence Length (Frames): 147\n",
      "Phrase length: 4\n",
      "\n",
      "--- Example 3 ---\n",
      "Landmarks shape: (211, 543, 3)\n",
      "Phrase: ['b', 'r', 'ū', 'c', 'e']\n",
      "Landmark Sequence Length (Frames): 211\n",
      "Phrase length: 5\n",
      "\n",
      "--- Example 4 ---\n",
      "Landmarks shape: (130, 543, 3)\n",
      "Phrase: ['o', 'z', 'o', 'n', 's']\n",
      "Landmark Sequence Length (Frames): 130\n",
      "Phrase length: 5\n",
      "\n",
      "--- Example 5 ---\n",
      "Landmarks shape: (247, 543, 3)\n",
      "Phrase: ['š', 'o', 'k', 'ē', 'j', 'o', 't']\n",
      "Landmark Sequence Length (Frames): 247\n",
      "Phrase length: 7\n",
      "\n",
      "--- Example 6 ---\n",
      "Landmarks shape: (7, 543, 3)\n",
      "Phrase: ['o']\n",
      "Landmark Sequence Length (Frames): 7\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 7 ---\n",
      "Landmarks shape: (230, 543, 3)\n",
      "Phrase: ['ū', 'd', 'e', 'n', 's']\n",
      "Landmark Sequence Length (Frames): 230\n",
      "Phrase length: 5\n",
      "\n",
      "--- Example 8 ---\n",
      "Landmarks shape: (37, 543, 3)\n",
      "Phrase: ['a']\n",
      "Landmark Sequence Length (Frames): 37\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 9 ---\n",
      "Landmarks shape: (162, 543, 3)\n",
      "Phrase: ['š', 'a', 'l', 'l', 'e']\n",
      "Landmark Sequence Length (Frames): 162\n",
      "Phrase length: 5\n",
      "\n",
      "--- Example 10 ---\n",
      "Landmarks shape: (188, 543, 3)\n",
      "Phrase: ['s', 'p', 'i', 'e', 'ž']\n",
      "Landmark Sequence Length (Frames): 188\n",
      "Phrase length: 5\n",
      "\n",
      "--- Example 11 ---\n",
      "Landmarks shape: (71, 543, 3)\n",
      "Phrase: ['3']\n",
      "Landmark Sequence Length (Frames): 71\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 12 ---\n",
      "Landmarks shape: (30, 543, 3)\n",
      "Phrase: ['ņ']\n",
      "Landmark Sequence Length (Frames): 30\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 13 ---\n",
      "Landmarks shape: (265, 543, 3)\n",
      "Phrase: ['ļ', 'o', 't', 'i', ' ', 's', 'ā', 'p']\n",
      "Landmark Sequence Length (Frames): 265\n",
      "Phrase length: 8\n",
      "\n",
      "--- Example 14 ---\n",
      "Landmarks shape: (173, 543, 3)\n",
      "Phrase: ['i', 'p', 'a', ' ', 'i', 'r']\n",
      "Landmark Sequence Length (Frames): 173\n",
      "Phrase length: 6\n",
      "\n",
      "--- Example 15 ---\n",
      "Landmarks shape: (16, 543, 3)\n",
      "Phrase: ['s']\n",
      "Landmark Sequence Length (Frames): 16\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 16 ---\n",
      "Landmarks shape: (91, 543, 3)\n",
      "Phrase: ['b', 'e', 't']\n",
      "Landmark Sequence Length (Frames): 91\n",
      "Phrase length: 3\n",
      "\n",
      "--- Example 17 ---\n",
      "Landmarks shape: (32, 543, 3)\n",
      "Phrase: ['k']\n",
      "Landmark Sequence Length (Frames): 32\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 18 ---\n",
      "Landmarks shape: (42, 543, 3)\n",
      "Phrase: ['8']\n",
      "Landmark Sequence Length (Frames): 42\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 19 ---\n",
      "Landmarks shape: (113, 543, 3)\n",
      "Phrase: ['d', 'a', 'u', 'z']\n",
      "Landmark Sequence Length (Frames): 113\n",
      "Phrase length: 4\n",
      "\n",
      "--- Example 20 ---\n",
      "Landmarks shape: (117, 543, 3)\n",
      "Phrase: ['0', 'h', '0']\n",
      "Landmark Sequence Length (Frames): 117\n",
      "Phrase length: 3\n",
      "\n",
      "--- Example 21 ---\n",
      "Landmarks shape: (178, 543, 3)\n",
      "Phrase: ['m', 'o', 'h', 'i', 't', 'o']\n",
      "Landmark Sequence Length (Frames): 178\n",
      "Phrase length: 6\n",
      "\n",
      "--- Example 22 ---\n",
      "Landmarks shape: (34, 543, 3)\n",
      "Phrase: ['2']\n",
      "Landmark Sequence Length (Frames): 34\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 23 ---\n",
      "Landmarks shape: (249, 543, 3)\n",
      "Phrase: ['10', ' ', 'k', 'u', 'ņ', 'ģ', 'i']\n",
      "Landmark Sequence Length (Frames): 249\n",
      "Phrase length: 7\n",
      "\n",
      "--- Example 24 ---\n",
      "Landmarks shape: (398, 543, 3)\n",
      "Phrase: ['ķ', 'ē', 'ķ', 'u', ' ', 'č', 'u', 'k', 's', 't', 'i']\n",
      "Landmark Sequence Length (Frames): 398\n",
      "Phrase length: 11\n",
      "\n",
      "--- Example 25 ---\n",
      "Landmarks shape: (41, 543, 3)\n",
      "Phrase: ['7']\n",
      "Landmark Sequence Length (Frames): 41\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 26 ---\n",
      "Landmarks shape: (40, 543, 3)\n",
      "Phrase: ['e']\n",
      "Landmark Sequence Length (Frames): 40\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 27 ---\n",
      "Landmarks shape: (8, 543, 3)\n",
      "Phrase: ['n']\n",
      "Landmark Sequence Length (Frames): 8\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 28 ---\n",
      "Landmarks shape: (39, 543, 3)\n",
      "Phrase: ['v']\n",
      "Landmark Sequence Length (Frames): 39\n",
      "Phrase length: 1\n",
      "\n",
      "--- Example 29 ---\n",
      "Landmarks shape: (187, 543, 3)\n",
      "Phrase: ['i', 'n', 'g', 'u', 's']\n",
      "Landmark Sequence Length (Frames): 187\n",
      "Phrase length: 5\n",
      "\n",
      "--- Example 30 ---\n",
      "Landmarks shape: (9, 543, 3)\n",
      "Phrase: ['4']\n",
      "Landmark Sequence Length (Frames): 9\n",
      "Phrase length: 1\n",
      "\n",
      "--- Verification Complete ---\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "TFRECORD_PATH = \"../data/processed_landmarks/training.tfrecord\" # Sagaidāmāis ceļš uz TFRecord failu\n",
    "print(f\"Verifying: {TFRECORD_PATH}\")\n",
    "try:\n",
    "    raw_dataset = tf.data.TFRecordDataset(TFRECORD_PATH)\n",
    "\n",
    "    print(\"\\n--- Verifying first 5 examples ---\")\n",
    "    for i, raw_record in enumerate(raw_dataset.take(30)):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "\n",
    "        landmarks_bytes = example.features.feature['landmarks'].bytes_list.value[0]\n",
    "        landmarks_flat = tf.io.decode_raw(landmarks_bytes, tf.float32)\n",
    "        length = example.features.feature['length'].int64_list.value[0]\n",
    "        phrase = example.features.feature['phrase'].int64_list.value\n",
    "        phrase_length = example.features.feature['phrase_length'].int64_list.value[0]\n",
    "\n",
    "        expected_size = length * NUM_NODES * LANDMARK_DIMS\n",
    "\n",
    "        if tf.size(landmarks_flat).numpy() != expected_size:\n",
    "            print(f\"ERROR: Decoded size mismatch!\")\n",
    "            print(f\"  Decoded flat tensor size: {tf.size(landmarks_flat).numpy()}\")\n",
    "            print(f\"  Expected size (length * NUM_NODES * LANDMARK_DIMS): {length} * {NUM_NODES} * {LANDMARK_DIMS} = {expected_size}\")\n",
    "            landmarks_shape = \"Error - Size Mismatch\"\n",
    "        else:\n",
    "            landmarks = tf.reshape(landmarks_flat, (length, NUM_NODES, LANDMARK_DIMS))\n",
    "            landmarks_shape = landmarks.shape\n",
    "            if np.isnan(landmarks.numpy()).any():\n",
    "                print(\"Warning: NaNs detected in decoded landmarks!\")\n",
    "            if np.isinf(landmarks.numpy()).any():\n",
    "                print(\"Warning: Infs detected in decoded landmarks!\")\n",
    "\n",
    "\n",
    "        print(\"Landmarks shape:\", landmarks_shape)\n",
    "        decoded_phrase = [id_to_char.get(idx, f\"Unknown({idx})\") for idx in phrase]\n",
    "        print(\"Phrase:\", decoded_phrase)\n",
    "        print(\"Landmark Sequence Length (Frames):\", length)\n",
    "        print(\"Phrase length:\", phrase_length)\n",
    "\n",
    "    print(\"\\n--- Verification Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during TFRecord verification: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
